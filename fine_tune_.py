# -*- coding: utf-8 -*-
"""Fine_tune .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13DNAw_UBWmm5OZQM7u3vi_6VT4DTZ1NZ
"""

pip install torch transformers datasets peft pypdf accelerate sentencepiece

from pypdf import PdfReader
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model
import torch

"""# Load my custom data(PDF FROM)"""

pdf_path = "/content/AI_and_Machine_Learning (1).pdf"
reader = PdfReader(pdf_path)

text = ""
for page in reader.pages:
    text += page.extract_text() + "\n"

print("PDF loaded!")

"""# split into chunks"""

chunks = text.split("\n\n")

data = []
for chunk in chunks:
    if len(chunk) > 100:
        data.append({
            "text": f"Explain this concept:\n{chunk}"
        })

dataset = Dataset.from_list(data)

"""# Load Model"""

from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

model.config.use_cache = False

dataset = Dataset.from_list(data)

def tokenize(example):
    return tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=256
    )

tokenized_dataset = dataset.map(tokenize, remove_columns=["text"])

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

training_args = TrainingArguments(
    output_dir="./fine_tuned_model",
    per_device_train_batch_size=1,
    num_train_epochs=2,
    learning_rate=2e-4,
    logging_steps=10
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)

trainer.train()

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import os

base_model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
adapter_path = "fine_tuned_model/checkpoint-20"   # LOCAL path

print("Files in adapter folder:", os.listdir(adapter_path))

# Load tokenizer from BASE model
tokenizer = AutoTokenizer.from_pretrained(base_model_name)

# Load base model
model = AutoModelForCausalLM.from_pretrained(base_model_name)

# Load LoRA adapter from LOCAL folder
model = PeftModel.from_pretrained(model, adapter_path)

model.eval()
print("Model loaded successfully!")

prompt = "Answer the following question clearly:\nWhat is Machine learning?"

inputs = tokenizer(prompt, return_tensors="pt")

with torch.no_grad():
    output = model.generate(
        **inputs,
        max_new_tokens=150,
        do_sample=True,
        temperature=0.7,
        top_p=0.9
    )

print("\nModel Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))




